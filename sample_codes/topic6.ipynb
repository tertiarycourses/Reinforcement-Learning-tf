{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8b3249-e6c6-4466-8b2b-30cc383ec9cd",
   "metadata": {},
   "source": [
    "# Topic 7 Advaned Stable Baseline3 Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf85fe2c-d09d-4912-ae6c-914a8420dd74",
   "metadata": {},
   "source": [
    "## Custom Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5dc79-e1cf-44ad-9c6d-bc43b0271003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5389d5-d2be-4819-aa8d-debc953be796",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcc5cb-4954-4e06-a0fa-2c328d8b52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log = \"data/tb/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f21b813-1eba-4bf1-828e-293010b162dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\"MlpPolicy\",\n",
    "            env,\n",
    "            verbose=1,\n",
    "            train_freq=16,\n",
    "            gradient_steps=8,\n",
    "            gamma=0.99,\n",
    "            exploration_fraction=0.2,\n",
    "            exploration_final_eps=0.07,\n",
    "            target_update_interval=600,\n",
    "            learning_starts=1000,\n",
    "            buffer_size=10000,\n",
    "            batch_size=128,\n",
    "            learning_rate=4e-3,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2791c-0983-487f-8176-91f5d122bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), deterministic=True, n_eval_episodes=20)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652baf1-2089-46fa-9063-b395776929b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Monitor training in tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $tensorboard_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6456277-a90d-4ce2-87d3-d58a8111174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(int(2e4), log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56eb03-4c73-408f-945e-f606115cc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"dqn_cartpole2\")\n",
    "del model # remove to demonstrate saving and loading\n",
    "model = DQN.load(\"dqn_cartpole2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced9c7b-26d0-4e4e-a64f-8e966b1eccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _state = model.predict(obs,deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845bee7-4e90-4659-ae95-97e7488739a8",
   "metadata": {},
   "source": [
    "### Activity: Custom Network Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51643ebe-2426-4b1e-bdb7-a667e4cacc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51754dc-7208-44b4-a840-340963972265",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db83f4e-15c8-4c75-849e-32252cda2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log = \"data/tb/dqn3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69fcf6-6672-42ab-91a2-1448ed6b2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\"MlpPolicy\",\n",
    "            env,\n",
    "            verbose=1,\n",
    "            train_freq=16,\n",
    "            gradient_steps=8,\n",
    "            gamma=0.99,\n",
    "            exploration_fraction=0.2,\n",
    "            exploration_final_eps=0.07,\n",
    "            target_update_interval=600,\n",
    "            learning_starts=1000,\n",
    "            buffer_size=10000,\n",
    "            batch_size=128,\n",
    "            learning_rate=4e-3,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e75ca-337f-46a2-8e8e-1bd9018dbbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), deterministic=True, n_eval_episodes=20)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce26cc-df33-40c6-9977-9182abc71a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Monitor training in tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $tensorboard_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a55a32-9e98-456a-8fb8-7404c73d75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(int(1e5), log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e051a9d-e8a2-42f1-a7c2-d71d559cf6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"dqn_lunar2\")\n",
    "del model # remove to demonstrate saving and loading\n",
    "model = DQN.load(\"dqn_lunar2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7ce0d0-242f-4d55-b6b1-9ee4d9d9bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _state = model.predict(obs,deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740893e-3388-4a84-9a7f-9bb9269db1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
