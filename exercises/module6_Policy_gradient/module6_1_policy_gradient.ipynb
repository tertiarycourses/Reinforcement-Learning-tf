{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poilicy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_model(env, hidden_layer_neurons, lr):\n",
    "    dimen = env.reset().shape\n",
    "    num_actions = env.action_space.n\n",
    "    inp = layers.Input(shape=dimen,name=\"input_x\")\n",
    "    adv = layers.Input(shape=[1], name=\"advantages\")\n",
    "    x = layers.Dense(hidden_layer_neurons, \n",
    "                     activation=\"relu\", \n",
    "                     use_bias=False,\n",
    "                     kernel_initializer=glorot_uniform(seed=42),\n",
    "                     name=\"dense_1\")(inp)\n",
    "    out = layers.Dense(num_actions, \n",
    "                       activation=\"softmax\", \n",
    "                       kernel_initializer=glorot_uniform(seed=42),\n",
    "                       use_bias=False,\n",
    "                       name=\"out\")(x)\n",
    "\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        # actual: 0 predict: 0 -> log(0 * (0 - 0) + (1 - 0) * (0 + 0)) = -inf\n",
    "        # actual: 1 predict: 1 -> log(1 * (1 - 1) + (1 - 1) * (1 + 1)) = -inf\n",
    "        # actual: 1 predict: 0 -> log(1 * (1 - 0) + (1 - 1) * (1 + 0)) = 0\n",
    "        # actual: 0 predict: 1 -> log(0 * (0 - 1) + (1 - 0) * (0 + 1)) = 0\n",
    "        log_lik = K.log(y_true * (y_true - y_pred) + (1 - y_true) * (y_true + y_pred))\n",
    "        return K.mean(log_lik * adv, keepdims=True)\n",
    "        \n",
    "    model_train = Model(inputs=[inp, adv], outputs=out)\n",
    "    model_train.compile(loss=custom_loss, optimizer=Adam(lr))\n",
    "    model_predict = Model(inputs=[inp], outputs=out)\n",
    "    return model_train, model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [2.9701, 1.99, 1]\n",
    "    \"\"\"\n",
    "    prior = 0\n",
    "    out = []\n",
    "    for val in r:\n",
    "        new_val = val + prior * gamma\n",
    "        out.append(new_val)\n",
    "        prior = new_val\n",
    "    return np.array(out[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants defining our neural network\n",
    "hidden_layer_neurons = 8\n",
    "gamma = .99\n",
    "dimen = len(env.reset())\n",
    "print_every = 100\n",
    "batch_size = 50\n",
    "num_episodes = 10000\n",
    "render = False\n",
    "lr = 1e-2\n",
    "goal = 190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See our trained bot in action\n",
    "def score_model(model, num_tests, render=False):\n",
    "    scores = []    \n",
    "    for num_test in range(num_tests):\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            state = np.reshape(observation, [1, dimen])\n",
    "            predict = model.predict([state])[0]\n",
    "            action = np.argmax(predict)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(reward_sum)\n",
    "    env.close()\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 21:20:41.366097 140734810432960 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0729 21:20:41.383095 140734810432960 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0729 21:20:41.401281 140734810432960 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0729 21:20:41.416873 140734810432960 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0729 21:20:41.423655 140734810432960 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_x (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 48\n",
      "Trainable params: 48\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_train, model_predict = get_policy_model(env, hidden_layer_neurons, lr)\n",
    "model_predict.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 21:20:44.375635 140734810432960 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for training episode 100: 23.06 Test Score: 10.20 Loss: 0.027904 \n",
      "Average reward for training episode 200: 21.40 Test Score: 10.60 Loss: 0.022190 \n",
      "Average reward for training episode 300: 22.86 Test Score: 10.60 Loss: 0.017080 \n",
      "Average reward for training episode 400: 24.44 Test Score: 63.90 Loss: 0.012880 \n",
      "Average reward for training episode 500: 25.76 Test Score: 102.60 Loss: 0.009790 \n",
      "Average reward for training episode 600: 27.17 Test Score: 137.50 Loss: 0.007683 \n",
      "Average reward for training episode 700: 28.80 Test Score: 133.70 Loss: 0.006227 \n",
      "Average reward for training episode 800: 27.64 Test Score: 122.40 Loss: 0.005383 \n",
      "Average reward for training episode 900: 26.81 Test Score: 127.80 Loss: 0.004832 \n",
      "Average reward for training episode 1000: 29.81 Test Score: 111.90 Loss: 0.004019 \n",
      "Average reward for training episode 1100: 27.06 Test Score: 140.30 Loss: 0.003751 \n",
      "Average reward for training episode 1200: 26.11 Test Score: 103.40 Loss: 0.003504 \n",
      "Average reward for training episode 1300: 26.87 Test Score: 97.90 Loss: 0.003124 \n",
      "Average reward for training episode 1400: 22.97 Test Score: 110.00 Loss: 0.002763 \n",
      "Average reward for training episode 1500: 25.06 Test Score: 97.30 Loss: 0.002411 \n",
      "Average reward for training episode 1600: 25.19 Test Score: 119.20 Loss: 0.002179 \n",
      "Average reward for training episode 1700: 26.96 Test Score: 127.00 Loss: 0.001985 \n",
      "Average reward for training episode 1800: 32.76 Test Score: 130.90 Loss: 0.001624 \n",
      "Average reward for training episode 1900: 25.87 Test Score: 118.50 Loss: 0.001371 \n",
      "Average reward for training episode 2000: 26.90 Test Score: 133.10 Loss: 0.001122 \n",
      "Average reward for training episode 2100: 27.37 Test Score: 143.60 Loss: 0.001024 \n",
      "Average reward for training episode 2200: 24.54 Test Score: 124.40 Loss: 0.000866 \n",
      "Average reward for training episode 2300: 27.02 Test Score: 127.30 Loss: 0.000623 \n",
      "Average reward for training episode 2400: 27.51 Test Score: 149.40 Loss: 0.000509 \n",
      "Average reward for training episode 2500: 26.79 Test Score: 120.10 Loss: 0.000562 \n",
      "Average reward for training episode 2600: 30.21 Test Score: 160.20 Loss: 0.000528 \n",
      "Average reward for training episode 2700: 25.60 Test Score: 141.80 Loss: 0.000524 \n",
      "Average reward for training episode 2800: 27.80 Test Score: 137.70 Loss: 0.000364 \n",
      "Average reward for training episode 2900: 25.80 Test Score: 118.80 Loss: 0.000282 \n",
      "Average reward for training episode 3000: 25.67 Test Score: 150.40 Loss: 0.000121 \n",
      "Average reward for training episode 3100: 25.42 Test Score: 140.30 Loss: 0.000030 \n",
      "Average reward for training episode 3200: 28.11 Test Score: 142.40 Loss: -0.000026 \n",
      "Average reward for training episode 3300: 27.35 Test Score: 127.50 Loss: -0.000142 \n",
      "Average reward for training episode 3400: 29.41 Test Score: 153.70 Loss: -0.000234 \n",
      "Average reward for training episode 3500: 27.34 Test Score: 144.80 Loss: -0.000361 \n",
      "Average reward for training episode 3600: 24.44 Test Score: 149.30 Loss: -0.000511 \n",
      "Average reward for training episode 3700: 24.96 Test Score: 153.00 Loss: -0.000551 \n",
      "Average reward for training episode 3800: 26.76 Test Score: 153.20 Loss: -0.000598 \n",
      "Average reward for training episode 3900: 26.93 Test Score: 133.40 Loss: -0.000749 \n",
      "Average reward for training episode 4000: 29.22 Test Score: 183.40 Loss: -0.000802 \n",
      "Average reward for training episode 4100: 25.51 Test Score: 148.70 Loss: -0.000771 \n",
      "Average reward for training episode 4200: 27.21 Test Score: 154.10 Loss: -0.000868 \n",
      "Average reward for training episode 4300: 24.61 Test Score: 140.80 Loss: -0.000936 \n",
      "Average reward for training episode 4400: 27.47 Test Score: 163.90 Loss: -0.001025 \n",
      "Average reward for training episode 4500: 29.48 Test Score: 133.00 Loss: -0.001053 \n",
      "Average reward for training episode 4600: 27.76 Test Score: 118.90 Loss: -0.001066 \n",
      "Average reward for training episode 4700: 26.22 Test Score: 133.70 Loss: -0.001097 \n",
      "Average reward for training episode 4800: 25.46 Test Score: 143.80 Loss: -0.001126 \n",
      "Average reward for training episode 4900: 26.69 Test Score: 111.40 Loss: -0.001191 \n",
      "Average reward for training episode 5000: 22.92 Test Score: 152.50 Loss: -0.001263 \n",
      "Average reward for training episode 5100: 24.78 Test Score: 142.60 Loss: -0.001916 \n",
      "Average reward for training episode 5200: 28.08 Test Score: 153.00 Loss: -0.002273 \n",
      "Average reward for training episode 5300: 27.25 Test Score: 161.90 Loss: -0.002481 \n",
      "Average reward for training episode 5400: 25.40 Test Score: 164.10 Loss: -0.002568 \n",
      "Average reward for training episode 5500: 27.61 Test Score: 184.80 Loss: -0.002611 \n",
      "Average reward for training episode 5600: 26.62 Test Score: 147.60 Loss: -0.002598 \n",
      "Average reward for training episode 5700: 25.73 Test Score: 168.50 Loss: -0.002586 \n",
      "Average reward for training episode 5800: 23.95 Test Score: 160.20 Loss: -0.002619 \n",
      "Average reward for training episode 5900: 27.40 Test Score: 170.30 Loss: -0.002719 \n",
      "Average reward for training episode 6000: 25.34 Test Score: 182.60 Loss: -0.002699 \n",
      "Average reward for training episode 6100: 25.24 Test Score: 183.40 Loss: -0.002775 \n",
      "Average reward for training episode 6200: 24.16 Test Score: 180.50 Loss: -0.002864 \n",
      "Average reward for training episode 6300: 25.28 Test Score: 165.50 Loss: -0.002908 \n",
      "Average reward for training episode 6400: 25.63 Test Score: 162.90 Loss: -0.002902 \n",
      "Average reward for training episode 6500: 26.56 Test Score: 171.10 Loss: -0.002938 \n",
      "Average reward for training episode 6600: 26.54 Test Score: 158.60 Loss: -0.002970 \n",
      "Average reward for training episode 6700: 28.08 Test Score: 180.60 Loss: -0.003034 \n",
      "Average reward for training episode 6800: 26.76 Test Score: 171.00 Loss: -0.002972 \n",
      "Average reward for training episode 6900: 27.58 Test Score: 177.20 Loss: -0.002987 \n",
      "Average reward for training episode 7000: 25.70 Test Score: 185.80 Loss: -0.002982 \n",
      "Average reward for training episode 7100: 27.12 Test Score: 175.50 Loss: -0.003024 \n",
      "Average reward for training episode 7200: 26.09 Test Score: 197.00 Loss: -0.003105 \n",
      "Solved in 7199 episodes!\n"
     ]
    }
   ],
   "source": [
    "reward_sum = 0\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Placeholders for our observations, outputs and rewards\n",
    "states = np.empty(0).reshape(0,dimen)\n",
    "actions = np.empty(0).reshape(0,1)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "# Setting up our environment\n",
    "observation = env.reset()\n",
    "\n",
    "num_episode = 0\n",
    "\n",
    "losses = []\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    # Append the observations to our batch\n",
    "    state = np.reshape(observation, [1, dimen])\n",
    "    \n",
    "    predict = model_predict.predict([state])[0]\n",
    "    action = np.random.choice(range(num_actions),p=predict)\n",
    "    \n",
    "    # Append the observations and outputs for learning\n",
    "    states = np.vstack([states, state])\n",
    "    actions = np.vstack([actions, action])\n",
    "    \n",
    "    # Determine the oucome of our action\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    \n",
    "    if done:\n",
    "        # Determine standardized rewards\n",
    "        discounted_rewards_episode = discount_rewards(rewards, gamma)       \n",
    "        discounted_rewards = np.vstack([discounted_rewards, discounted_rewards_episode])\n",
    "        \n",
    "        rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "        if (num_episode + 1) % batch_size == 0:\n",
    "            discounted_rewards -= discounted_rewards.mean()\n",
    "            discounted_rewards /= discounted_rewards.std()\n",
    "            discounted_rewards = discounted_rewards.squeeze()\n",
    "            actions = actions.squeeze().astype(int)\n",
    "           \n",
    "            actions_train = np.zeros([len(actions), num_actions])\n",
    "            actions_train[np.arange(len(actions)), actions] = 1\n",
    "            \n",
    "            loss = model_train.train_on_batch([states, discounted_rewards], actions_train)\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Clear out game variables\n",
    "            states = np.empty(0).reshape(0,dimen)\n",
    "            actions = np.empty(0).reshape(0,1)\n",
    "            discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "\n",
    "        # Print periodically\n",
    "        if (num_episode + 1) % print_every == 0:\n",
    "            # Print status\n",
    "            score = score_model(model_predict,10)\n",
    "            print(\"Average reward for training episode {}: {:0.2f} Test Score: {:0.2f} Loss: {:0.6f} \".format(\n",
    "                (num_episode + 1), reward_sum/print_every, \n",
    "                score,\n",
    "                np.mean(losses[-print_every:])))\n",
    "            \n",
    "            if score >= goal:\n",
    "                print(\"Solved in {} episodes!\".format(num_episode))\n",
    "                break\n",
    "            reward_sum = 0\n",
    "                \n",
    "        num_episode += 1\n",
    "        observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
